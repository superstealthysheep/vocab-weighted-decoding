<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Weighted Decoding for Vocabulary Acquisition | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Weighted Decoding for Vocabulary Acquisition</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">NLP Class Project: CSCI 5541, Fall 24 - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Language Learning Mentors (LLMs)</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="./files/members/william.jpg">
            
            
          </div>
          <p>
                        
            William Walker
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="./files/members/emilie.png">
            
          </div>
          <p>
            
            Emilie Bourget
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="./files/members/philip.jpg">            
            
          </div>
          <p>
              Philip Nguyen
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="https://github.com/superstealthysheep/vocab-weighted-decoding"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <!-- <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>               -->
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>


    <!-- <b>One or two sentences on the motivation behind the problem you are solving.</b> -->
    <!-- <p>When beginner-level language learners try to practice conversation with an LLM, a frequent frustration is that the LLM uses a large amount of complicated, advanced vocabulary. We intend to address this problem by creating a conversational chatbot that will preferentially use words familiar to the user and will expand the user's English vocabulary by gradually introducing novel words. Given a list of words that the user is familiar with and a list that is on the frontier of their understanding, at inference time the model will select its wording to prefer familiar words and a small amount of unfamiliar words. -->

    <!-- Our project is to create a conversational chatbot that will expand the user's English vocabulary by gradually introducing novel words. Given a list of words that the user is familiar with and a list that is on the frontier of their understanding, at inference time the model will select its wording to prefer familiar words and a small amount of unfamiliar words. -->

    <!-- <b>One or two sentences describing the approach you took.</b> -->
    <!-- The core of our model will be Llama 3.1 8B. Our problem fits under the larger umbrella of controllable text generation, and the approach that we currently plan to use is to modify the decoder to take a list of target words (e.g. familar words, frontier words) as an auxillary input. Then, when sampling, the decoder will  boost the logits of words* in the familiar and frontier vocab lists. (*Note: since sampling happens on the subword token level, we are thinking about employing a tree search method such as beam search to "look ahead" several tokens into the future and boost the probability of any paths resulting in desired words.) -->


    <!-- <b>One or two sentences on the motivation behind the problem you are solving.</b>
    <p>Our project is to create a <u>conversational chatbot</u> that will expand the user's English vocabulary by <u>gradually introducing novel words</u>. Given a list of words that the user is familiar with and a list that is on the frontier of their understanding, at inference time the model will select its wording to <u>prefer familiar words</u> and a small amount of unfamiliar words.</p>

    <b>One or two sentences describing the approach you took.</b>
    <p>The core of our model will be <u>Llama 3.1 8B</u>. Our problem fits under the larger umbrella of controllable text generation, and the approach that we currently plan to use is to <u>modify the decoder</u> to take a list of target words (e.g. familar words, frontier words) as an auxillary input. Then, when sampling, the decoder will <u>boost the logits</u> of words* in the familiar and frontier vocab lists. (*Note: since sampling happens on the subword token level, we are thinking about employing a tree search method such as beam search to "look ahead" several tokens into the future and boost the probability of any paths resulting in desired words.)</p>
    
    <b>One or two sentences on the main result you obtained.</b>
    <p>N/A</p> -->

    <p>
      Language learning is a complex process, and with the lack of personalized tools outside the classroom, language learners often are hindered in their pursuit of new vocabulary acquisition. Large language models (LLMs) present a promising medium for practical conversational practice as well as personalization.
    </p>
    <p>
      Our goal for this project is to aid in a language learner‚Äôs acquisition of new vocabulary words through conversation. More specifically, given a vocabulary word(s) the learner wants to learn, the LLM will generate a sentence containing the word with enough context for the learner to understand the meaning while keeping the skill level (known vocabulary) of the learner in mind. This allows the language learner to learn their desired vocabulary through sentences of their skill level.
    </p>
    <p>
      To do this, we utilize Meta‚Äôs Llama model and attempt various methods of sentence generation including prompting as well as decoder modifications. To evaluate the sentences, currently we are only evaluating for the coherence of the sentences, but ideally this system would need human evaluations since the sentence generation is user specific.
    </p>
    <p>
      For the scope of this class project, we have chosen to work with the english language since all team members are proficient in English it would better allow up to prototype and perform initial human evaluations.

    </p>




<hr>

<!-- <h2 id="teaser">Teaser Figure</h2>

<p>A figure that conveys the main idea behind the project or the main application being addressed. </p>

<p class="sys-img"><img src="./files/teaser.png" alt="imgname"></p>


<h3 id="the-timeline-and-the-highlights">Any subsection</h3>

<p>If you need to explain more about your figure</p>

<hr> -->

<h2 id="introduction">Introduction / Background / Motivation</h2>

<!-- <p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
As language learners, the authors have each attempted to engage in conversations with LLMs to practice our language skills. However, a common problem we've encountered is that the LLM uses lots of vocabulary that is too advanced for us to understand. The ultimate goal of this project is to create a general framework that will address this problem by (note: future features) monitoring the vocabulary used by and understood by the user and (note: current goal) preferentially use words understood by the user, limiting the amount of advanced vocabulary for a better pedagogical experience. For our project, we're limiting our scope to the English language since we all understand English and think it will serve as a good initial prototyping and proving ground.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
Based on a brief literature survey, there seems to not be much work directly done on the problem of limiting vocab for language learners. We have found writings on constrained decoding and looser ways to control the generation of LLMs, and we also intend to research further in the educational/HCI literature for other projects that seek to assist language (and especially vocabulary) aquisition.
<p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
This project would have a direct use for the authors (and other language learners) because it would allow for more effective pedagogical conversations with LLMs. This would potentially have general applications for all language learners who wish to practice conversation with LLMs but who are overwhelmed by a large amount of advanced vocabulary used by the LLM. The authors currently imagine that the user would likely be people learning a second language, but perhaps this technique could even find use in early acquisition of first languages in children (perhaps).
</p>

<hr> -->
<p>

</p>

<!-- <h2 id="approach">Approach</h2> -->

<!-- <p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
  The core of our model will be Llama 3.1 8B. Our problem fits under the larger umbrella of controllable text generation, and the approach that we currently plan to use is to modify the decoder to take a list of target words (e.g. familar words, frontier words) as an auxillary input. Then, when sampling, the decoder will  boost the logits of words* in the familiar and frontier vocab lists. (*Note: since sampling happens on the subword token level, we are thinking about employing a tree search method such as beam search to "look ahead" several tokens into the future and boost the probability of any paths resulting in desired words. I realize now that a good approach would also be to pre-tokenize our desired vocab lists and form them into a trie.) This vocab-list-based softly-constrained decoder seems novel in the literature based on our cursory investigations.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
Getting things running on MSI may be a challenge, and we're still at time of writing figuring out exactly how we're going to quantitatively evaluate our model. Another worry is that direct decoding-time interference may be too "local" of a modification to make, so maybe this method would not perform well in the case of languages with long-range linguistic structures (seperable verb/complement pairs like in German (and English) come to mind.)
</p> -->
<p>
In language learning, one of the most effective
ways to acquire fluency is through conversational
practice. One of the problems in language learning
is that predefined ‚Äúlevels‚Äù of proficiency of a language do not generalize well enough to an individual learning the language. Oftentimes the learning
path of a language learner outside the classroom
cannot be expressed as a linear progression through
predefined ‚Äúlevels‚Äù. Depending on individual interest factors for learning a language such as travel,
casual conversation, or mastery, different vocabulary may be more relevant at different times.
</p>
<p>
When the vocabulary in a conversation is too advanced, language learners are forced to rely on external aids such as dictionaries and translation tools.
This process disrupts the flow of practice and may
introduce even more advanced words that can lead
the language learner to an overwhelming exposure
of unfamiliar words. This is especially problematic
for language learners who are just starting or have
a limited vocabulary base. This disruption from the
flow of practice can lead to discouragement and
disengagement from the language learning process.
The goal of this project is to address the difference
in an individual‚Äôs vocabulary to make the flow of
practice more natural with less disruptions.
</p>
<p>
LLMs, with their ability to generate diverse and
context rich responses, hold promise to become
great assets for language learners. They are able to
simulate a conversational partner, and through this
project‚Äôs exploration of various sentence generation
techniques.
</p>
<p>
In this report, we present the results of our exploration of various methods for sentence generation
that may have application in the domain of language learning. We intend for this project to be the
first step towards towards eventually building an
LLM conversation partner that actively monitors
the user‚Äôs language proficiency and vocabulary and
tailors its own language and behavior to challenge
the language learner in a level-appropriate manner.
</p>


<hr>
<h2 id="approach">Approach</h2>
<p>
Our experiment tested the lexical logit boosting (LLB) prototype, focusing on generating and evaluating example sentences with specific vocabulary words. The vocabulary set consisted of the 500 most common English words scraped from a website that did not lemmatize words. This ensured words like "is" were included, unlike in lemmatized lists where they would be grouped under "be." The choice of this vocabulary set was arbitrary and motivated by its simplicity and suitability for prototyping, although other sets (e.g., words starting with "e") could have been used.
</p>
<p>
From this list, 20 target words were randomly selected. Various configurations of large language models (LLMs) were then prompted to generate example sentences using these target words. The Llama 3.1 8B Instruct model was tested with different values of the boost hyperparameter 
ùëè
, where 
ùëè = 0 served as the control. The Gemini 1.5 Flash model, accessed via Google‚Äôs API, was used as a reference state-of-the-art unmodified model.
</p>
<p>
Unlike the Llama models, which were prompted only with the target word and instructions, the Gemini model was supplied with the entire vocabulary list via its prompt. This approach, though effective for small vocabularies, is less scalable and potentially costly for larger vocabularies. The generated sentences were evaluated to produce the final results. Details on the prompts and hyperparameters are provided in Appendix A of the paper.
</p>





<hr>
<h2 id="results">Results</h2>

<p>
  The results demonstrated that stronger boosts led to increased adherence to the vocabulary set (Figure 3). However, this came at the cost of generation quality, as observed in human evaluations (Figure 5) and the LLM‚Äôs ability to follow prompts and include target words (Figure 4). This tradeoff between control and quality aligns with findings from Liang et al. (2024). Among the tested configurations, the moderate boost value 
  (ùëè = 4) provided the best balance: its quality was only slightly worse than the control 
  (ùëè = 0) and the Gemini model, while its mean percentage of non-vocabulary words was significantly lower than the Gemini model.
</p>
<p>
    Inter-evaluator agreement, measured using Krippendorff‚Äôs 
    ùõº, showed values of 0.76, 0.66, and 0.69 for the mechanics, semantics, and context metrics, respectively. Qualitatively, higher boost values 
    (ùëè > 4) tended to produce long, run-on sentences, likely because the vocabulary set excluded punctuation, leaving punctuation tokens unboosted. Adding punctuation to the vocabulary set could address this issue due to LLB's transparency properties.
    
    Additionally, the LLB model struggled to generate high-quality sentences when target words were lexically complex, suggesting a limitation in handling more challenging vocabulary.
</p>

<div style="display: flex; gap: 20px;">
  <figure>
    <img src="img/out_of_vocab.png" alt="alternatetext" style="width: 100%; height: auto;">
    <figcaption>
      Figure 3: As the boost parameter ùëè was increased, the
      generation tended more and more strongly towards using words from the vocab set, as intended.
    </figcaption>
  </figure>
  <figure>
    <img src="img/contains_target.png" alt="alternatetext" style="width: 100%; height: auto;">
    <figcaption>
      Figure 4: As the boost parameter ùëè was increased, the
      fraction of generations containing the target word undesirably decreased.
    </figcaption>
  </figure>
  <figure>
    <img src="img/human_evals.png" alt="alternatetext" style="width: 100%; height: auto;">
    <figcaption>
      Figure 5: As the boost parameter ùëè was increased, human evaluations of generation quality were initially not
    heavily affected. However, when ùëè became sufficiently
    large, evaluation scores dropped heavily.
    </figcaption>
  </figure>
</div>

<hr>
<h2 id="conclusion">Conclustion and Future Work</h2>



<!-- </p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Sentence</strong></td>
      <td style="text-align: center">Example 1</td>
      <td style="text-align: center">Example 2</td>
      <td style="text-align: center">Example 3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Errors</strong></td>
      <td style="text-align: center">error A, error B, error C</td>
      <td style="text-align: center">error C</td>
      <td style="text-align: center">error B</td>
    </tr>
  </tbody>
  <caption>Table 1. This is Table 1's caption</caption>
</table>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./files/results.png">
</div> -->
<br><br>

<hr>



<!-- <h2 id="conclusion">Conclusion and Future Work</h2>
<p>

  How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research?</p>

<hr> -->


  </div>
  


</body></html>
