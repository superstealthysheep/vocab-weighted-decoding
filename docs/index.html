<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Weighted Decoding for Vocabulary Acquisition | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Weighted Decoding for Vocabulary Acquisition</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">NLP Class Project: CSCI 5541, Fall 24 - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Language Learning Mentors (LLMs)</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="./files/members/william.jpg">
            
            
          </div>
          <p>
                        
            William Walker
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="./files/members/emilie.png">
            
          </div>
          <p>
            
            Emilie Bourget
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="./files/members/philip.jpg">            
            
          </div>
          <p>
              Philip Nguyen
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="https://github.com/superstealthysheep/vocab-weighted-decoding"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <!-- <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>               -->
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>


    <!-- <b>One or two sentences on the motivation behind the problem you are solving.</b> -->
    <!-- <p>When beginner-level language learners try to practice conversation with an LLM, a frequent frustration is that the LLM uses a large amount of complicated, advanced vocabulary. We intend to address this problem by creating a conversational chatbot that will preferentially use words familiar to the user and will expand the user's English vocabulary by gradually introducing novel words. Given a list of words that the user is familiar with and a list that is on the frontier of their understanding, at inference time the model will select its wording to prefer familiar words and a small amount of unfamiliar words. -->

    <!-- Our project is to create a conversational chatbot that will expand the user's English vocabulary by gradually introducing novel words. Given a list of words that the user is familiar with and a list that is on the frontier of their understanding, at inference time the model will select its wording to prefer familiar words and a small amount of unfamiliar words. -->

    <!-- <b>One or two sentences describing the approach you took.</b> -->
    <!-- The core of our model will be Llama 3.1 8B. Our problem fits under the larger umbrella of controllable text generation, and the approach that we currently plan to use is to modify the decoder to take a list of target words (e.g. familar words, frontier words) as an auxillary input. Then, when sampling, the decoder will  boost the logits of words* in the familiar and frontier vocab lists. (*Note: since sampling happens on the subword token level, we are thinking about employing a tree search method such as beam search to "look ahead" several tokens into the future and boost the probability of any paths resulting in desired words.) -->


    <!-- <b>One or two sentences on the motivation behind the problem you are solving.</b>
    <p>Our project is to create a <u>conversational chatbot</u> that will expand the user's English vocabulary by <u>gradually introducing novel words</u>. Given a list of words that the user is familiar with and a list that is on the frontier of their understanding, at inference time the model will select its wording to <u>prefer familiar words</u> and a small amount of unfamiliar words.</p>

    <b>One or two sentences describing the approach you took.</b>
    <p>The core of our model will be <u>Llama 3.1 8B</u>. Our problem fits under the larger umbrella of controllable text generation, and the approach that we currently plan to use is to <u>modify the decoder</u> to take a list of target words (e.g. familar words, frontier words) as an auxillary input. Then, when sampling, the decoder will <u>boost the logits</u> of words* in the familiar and frontier vocab lists. (*Note: since sampling happens on the subword token level, we are thinking about employing a tree search method such as beam search to "look ahead" several tokens into the future and boost the probability of any paths resulting in desired words.)</p>
    
    <b>One or two sentences on the main result you obtained.</b>
    <p>N/A</p> -->

    <p>
      Language learning is a complex process, and with the lack of personalized tools outside the classroom, language learners often are hindered in their pursuit of new vocabulary acquisition. Large language models (LLMs) present a promising medium for practical conversational practice as well as personalization. Our goal for this project is to aid in a language learner’s acquisition of new vocabulary words through conversation. More specifically, given a vocabulary word(s) the learner wants to learn, the LLM will generate a sentence containing the word with enough context for the learner to understand the meaning while keeping the skill level (known vocabulary) of the learner in mind. This allows the language learner to learn their desired vocabulary through sentences of their skill level. To do this, we utilize Meta’s Llama model and attempt various methods of sentence generation including prompting as well as decoder modifications. To evaluate the sentences, currently we are only evaluating for the coherence of the sentences, but ideally this system would need human evaluations since the sentence generation is user specific. For the scope of this class project, we have chosen to work with the english language since all team members are proficient in English it would better allow up to prototype and perform initial human evaluations.

    </p>




<hr>

<!-- <h2 id="teaser">Teaser Figure</h2>

<p>A figure that conveys the main idea behind the project or the main application being addressed. </p>

<p class="sys-img"><img src="./files/teaser.png" alt="imgname"></p>


<h3 id="the-timeline-and-the-highlights">Any subsection</h3>

<p>If you need to explain more about your figure</p>

<hr> -->

<h2 id="introduction">Introduction / Background / Motivation</h2>

<!-- <p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
As language learners, the authors have each attempted to engage in conversations with LLMs to practice our language skills. However, a common problem we've encountered is that the LLM uses lots of vocabulary that is too advanced for us to understand. The ultimate goal of this project is to create a general framework that will address this problem by (note: future features) monitoring the vocabulary used by and understood by the user and (note: current goal) preferentially use words understood by the user, limiting the amount of advanced vocabulary for a better pedagogical experience. For our project, we're limiting our scope to the English language since we all understand English and think it will serve as a good initial prototyping and proving ground.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
Based on a brief literature survey, there seems to not be much work directly done on the problem of limiting vocab for language learners. We have found writings on constrained decoding and looser ways to control the generation of LLMs, and we also intend to research further in the educational/HCI literature for other projects that seek to assist language (and especially vocabulary) aquisition.
<p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
This project would have a direct use for the authors (and other language learners) because it would allow for more effective pedagogical conversations with LLMs. This would potentially have general applications for all language learners who wish to practice conversation with LLMs but who are overwhelmed by a large amount of advanced vocabulary used by the LLM. The authors currently imagine that the user would likely be people learning a second language, but perhaps this technique could even find use in early acquisition of first languages in children (perhaps).
</p>

<hr> -->
<p>

</p>

<!-- <h2 id="approach">Approach</h2> -->

<!-- <p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
  The core of our model will be Llama 3.1 8B. Our problem fits under the larger umbrella of controllable text generation, and the approach that we currently plan to use is to modify the decoder to take a list of target words (e.g. familar words, frontier words) as an auxillary input. Then, when sampling, the decoder will  boost the logits of words* in the familiar and frontier vocab lists. (*Note: since sampling happens on the subword token level, we are thinking about employing a tree search method such as beam search to "look ahead" several tokens into the future and boost the probability of any paths resulting in desired words. I realize now that a good approach would also be to pre-tokenize our desired vocab lists and form them into a trie.) This vocab-list-based softly-constrained decoder seems novel in the literature based on our cursory investigations.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
Getting things running on MSI may be a challenge, and we're still at time of writing figuring out exactly how we're going to quantitatively evaluate our model. Another worry is that direct decoding-time interference may be too "local" of a modification to make, so maybe this method would not perform well in the case of languages with long-range linguistic structures (seperable verb/complement pairs like in German (and English) come to mind.)
</p> -->
<p>
  In language learning, one of the most effective
ways to acquire fluency is through conversational
practice. One of the problems in language learning
is that predefined “levels” of proficiency of a language do not generalize well enough to an individual learning the language. Oftentimes the learning
path of a language learner outside the classroom
cannot be expressed as a linear progression through
predefined “levels”. Depending on individual interest factors for learning a language such as travel,
casual conversation, or mastery, different vocabulary may be more relevant at different times.
</p>
<p>
When the vocabulary in a conversation is too advanced, language learners are forced to rely on external aids such as dictionaries and translation tools.
This process disrupts the flow of practice and may
introduce even more advanced words that can lead
the language learner to an overwhelming exposure
of unfamiliar words. This is especially problematic
for language learners who are just starting or have
a limited vocabulary base. This disruption from the
flow of practice can lead to discouragement and
disengagement from the language learning process.
The goal of this project is to address the difference
in an individual’s vocabulary to make the flow of
practice more natural with less disruptions.
</p>
<p>
LLMs, with their ability to generate diverse and
context rich responses, hold promise to become
great assets for language learners. They are able to
simulate a conversational partner, and through this
project’s exploration of various sentence generation
techniques.
</p>
<p>
In this report, we present the results of our exploration of various methods for sentence generation
that may have application in the domain of language learning. We intend for this project to be the
first step towards towards eventually building an
LLM conversation partner that actively monitors
the user’s language proficiency and vocabulary and
tailors its own language and behavior to challenge
the language learner in a level-appropriate manner.
</p>

<hr>
    
<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>
We still need to figure our evaluation metrics out. We'll look at educational/HCI literature and perhaps end up defining some bespoke evaluation metric, though considering our time limitations, it will likely need to be quite simple to implement.
<!-- </p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Sentence</strong></td>
      <td style="text-align: center">Example 1</td>
      <td style="text-align: center">Example 2</td>
      <td style="text-align: center">Example 3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Errors</strong></td>
      <td style="text-align: center">error A, error B, error C</td>
      <td style="text-align: center">error C</td>
      <td style="text-align: center">error B</td>
    </tr>
  </tbody>
  <caption>Table 1. This is Table 1's caption</caption>
</table>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./files/results.png">
</div> -->
<br><br>

<hr>



<!-- <h2 id="conclusion">Conclusion and Future Work</h2>
<p>

  How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research?</p>

<hr> -->


  </div>
  


</body></html>
